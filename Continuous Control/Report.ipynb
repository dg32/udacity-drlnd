{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: Continuous Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem Statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem in this project is to learn the optimal policy in a model-free, continuous control task.  We use the Unity `Reacher` environment, in which a double-jointed robot arm can move to target locations. A reward of `+0.1` is obtained for each timestep that the agent's hand is in the goal location.  The goal of the agent therefore is to maintain its position at the target location for as many time steps as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The State and Action Spaces\n",
    "\n",
    "- The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.\n",
    "\n",
    "- Each action is a vector with four numbers, corresponding to torque applicable to two joints.\n",
    "\n",
    "- Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "- This task utilises a single agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. The Learning algorithm.\n",
    "\n",
    "Purely policy based methods have high variance, since they are based on Monte Carlo estimates.  Temporal difference, value-based methods are biased, but have lower variance.  Actor-critic methods are an attempt to reduce the variance of policy based methods, at the cost of introducing some bias.\n",
    "\n",
    "Deep Deterministic Policy Gradient (DDPG) is an algorithm applicable to continuous action spaces.  It is regarded as an actor-critic method, since it utilizes two neural networks.  One is used to update the policy, the other is used to evaluate the policy.  That is, rather than using the reward provided by the environment to compute a gradient update step, the value provided by the critic network is used instead to make the policy update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Architecture.\n",
    "\n",
    "The actor network contains three fully connected hidden layers, of `600`, `400` and `200` nodes respectively.  The hidden layers use `ReLU` activation functions.  The final layer maps to the action space, so uses a `tanh` activation function.\n",
    "\n",
    "The critic network contains two fully connected layers, of `400` and `300` nodes respectively.  The hidden layers use `ReLU` activation functions, while the output layer uses the identity function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implementation Details.\n",
    "\n",
    "- The implementation makes use of a replay buffer to decorrelate the observed experiences.\n",
    "\n",
    "\n",
    "- We use target networks with soft updates help stabilise the training.\n",
    "\n",
    "\n",
    "- Ornstein-Uhlenbeck noise is added to the action space, to aid exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Hyperparameters.\n",
    "\n",
    "The DDPG agent is trained with the following hyperparameters:\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size.\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128        # minibatch size for training.\n",
    "\n",
    "\n",
    "GAMMA = 0.99            # discount factor.\n",
    "\n",
    "\n",
    "TAU = 1e-3              # for soft update of target parameters.\n",
    "\n",
    "\n",
    "LR_ACTOR = 1.5e-4       # learning rate of the actor network.\n",
    "\n",
    "\n",
    "LR_CRITIC = 1.5e-4      # learning rate of the critic network.\n",
    "\n",
    "\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay.\n",
    "\n",
    "\n",
    "mu = 0                  # drift parameter for Ornstein-Uhlenbeck noise process.\n",
    "\n",
    "\n",
    "sigma = 0.2             # variance parameter for noise process.\n",
    "\n",
    "\n",
    "theta = 0.15            # mean reversion parameter for noise process.\n",
    "\n",
    "\n",
    "n_episodes = 400        # maximum number of episodes to run.\n",
    "\n",
    "\n",
    "max_t = 1000            # maximum number of timesteps per episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  Results.\n",
    "\n",
    "The DDPG algorithm succeeded in solving the environment in 269 episodes, with an average score of 30.05.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  Suggestions For Improvement.\n",
    "\n",
    "- The DDPG algorithm may be improved by using prioritized experience replay.\n",
    "\n",
    "\n",
    "- While this particular implementation solved the task, it is computationally expensive.  It is possible that optimizing the hyperparameters in a systematic way could improve convergence.\n",
    "\n",
    "\n",
    "- Noise can be added to the network parameters rather than to the action space.\n",
    "\n",
    "\n",
    "- The Q-prop algorithm is claimed to improve stability over DDPG.  It is a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate and combines the benefits of on-policy and off-policy methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
